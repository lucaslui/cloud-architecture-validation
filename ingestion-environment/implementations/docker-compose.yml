services:
  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    environment:
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,EXTERNAL://:19092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:19092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=1
    ports:
      - "9092:9092"
      - "19092:19092"
    volumes:
      - kafka_data:/bitnami/kafka
    healthcheck:
      test: ["CMD", "bash", "-lc", "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1"]
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 60s
      
  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      DYNAMIC_CONFIG_ENABLED: 'false'
    depends_on:
      kafka:
        condition: service_healthy

  redis:
    image: redis:7-alpine
    container_name: redis-hems
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    ports:
      - "6380:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 3s
      timeout: 2s
      retries: 20
  
  vernemq:
    image: vernemq/vernemq:latest
    container_name: vernemq
    environment:
      - DOCKER_VERNEMQ_ACCEPT_EULA=yes
      - DOCKER_VERNEMQ_ALLOW_ANONYMOUS=off
      - DOCKER_VERNEMQ_LISTENER__TCP__LOCALHOST=0.0.0.0:1883
      - DOCKER_VERNEMQ_LOG__CONSOLE__LEVEL=info
      # Enable plugins
      - DOCKER_VERNEMQ_PLUGINS__VMQ_DIVERSITY=on
      - DOCKER_VERNEMQ_PLUGINS__VMQ_PASSWD=off
      - DOCKER_VERNEMQ_PLUGINS__VMQ_ACL=off
      # Authentication with Redis
      - DOCKER_VERNEMQ_VMQ_DIVERSITY__AUTH_REDIS__ENABLED=on
      - DOCKER_VERNEMQ_VMQ_DIVERSITY__REDIS__HOST=redis
      - DOCKER_VERNEMQ_VMQ_DIVERSITY__REDIS__PORT=6379
      - DOCKER_VERNEMQ_VMQ_DIVERSITY__REDIS__DATABASE=0
    ports:
      - "1883:1883"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "timeout 2 bash -c '</dev/tcp/127.0.0.1/1883'"]
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 60s

  influxdb:
    image: influxdb:2.7
    container_name: influxdb
    ports:
      - "8086:8086"
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=admin123
      - DOCKER_INFLUXDB_INIT_ORG=hems
      - DOCKER_INFLUXDB_INIT_BUCKET=hems_timeseries
      # Token de desenvolvimento (use um segredo de verdade em prod)
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=dev-token
    volumes:
      - influxdb_data:/var/lib/influxdb2
    healthcheck:
      test: ["CMD", "influx", "ping", "--host", "http://localhost:8086"]
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 30s

  minio:
    image: minio/minio:latest
    container_name: minio
    command: server --console-address ":9001" /data
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9000/minio/health/ready >/dev/null || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 20s

  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      set -e;
      mc alias set local http://minio:9000 minioadmin minioadmin;
      mc mb -p local/hems-datalake || true;
      exit 0;
      "

  # =========================================================================================================

  schema-loader:
    build: ./schema-registry
    container_name: schema-loader
    restart: "no"
    environment:
      - REDIS_ADDR=redis:6379
      - SCHEMA_REGISTRY_FS_ROOT=/app
      - SCHEMA_TTL_SECONDS=0
    depends_on:
      redis:
        condition: service_healthy
    entrypoint: ["/app/schema-loader"]

  cred-perm-loader:
    build: ./cred-perm-loader
    container_name: cred-perm-loader
    restart: "no"
    environment:
      - REDIS_ADDR=redis:6379
      - AUTH_PASSWORD=minha-senha-forte
      - TELEMETRY_TOPIC=ingestion/telemetry
      - COMMANDS_TOPIC_PREFIX=ingestion/commands
      - START_ID=1
      - END_ID=10000
      - BATCH_SIZE=2000
      - COLLECTOR_USERNAME=hems-collector
      - COLLECTOR_PASSWORD=senha-coletor
    depends_on:
      redis:
        condition: service_healthy
    entrypoint: ["/app/cred-perm-loader"]

  # =========================================================================================================

  collector:
    build: ./collector
    container_name: collector
    restart: unless-stopped
    environment:
      - MQTT_BROKER_URL=tcp://vernemq:1883
      - MQTT_CLIENT_ID=hems-collector
      - MQTT_TOPIC=ingestion/telemetry
      - MQTT_QOS=0
      - MQTT_CHANNEL_DEPTH=20000
      - MQTT_USERNAME=hems-collector
      - MQTT_PASSWORD=senha-coletor
      # =============================
      - KAFKA_BROKERS=kafka:9092
      - KAFKA_TOPIC=raw-events-topic
      - KAFKA_DLQ_TOPIC=raw-events-dlq
      - KAFKA_TOPIC_PARTITIONS=6
      - KAFKA_DLQ_PARTITIONS=1
      - KAFKA_REPLICATION_FACTOR=1 # 2 ou 3 em prod
      - KAFKA_BATCH_SIZE=5000
      - KAFKA_BATCH_BYTES=4194304 # 4MB
      - KAFKA_BATCH_TIMEOUT_MS=25
      - KAFKA_COMPRESSION=snappy
      - KAFKA_REQUIRED_ACKS=one
      - KAFKA_MAX_ATTEMPTS=10
      - KAFKA_RETENTION_MS=86400000
      # =============================
      - PROCESSING_WORKERS=8
      # =============================
      - WORKER_QUEUE_SIZE=50000
      - WORK_QUEUE_STRATEGY=block
      - WORK_QUEUE_ENQ_TIMEOUT_MS=100
      # =============================
      - DISPATCHER_CAPACITY=50000
      - DISPATCHER_MAX_BATCH=5000
      - DISPATCHER_TICK_MS=20
    depends_on:
      vernemq:
        condition: service_healthy
      kafka:
        condition: service_healthy
      cred-perm-loader:
        condition: service_completed_successfully

  validator:
    build: ./validator
    container_name: validator
    restart: unless-stopped
    environment:
      - KAFKA_BROKERS=kafka:9092
      - KAFKA_GROUP_ID=validator
      - KAFKA_READER_TOPIC=raw-events-topic
      - KAFKA_WRITER_TOPIC=validated-events-topic
      - KAFKA_DLQ_TOPIC=validated-events-dlq
      - KAFKA_TOPIC_PARTITIONS=6
      - KAFKA_DLQ_PARTITIONS=1
      - KAFKA_RETENTION_MS=86400000
      - KAFKA_COMPRESSION=snappy
      - KAFKA_REPLICATION_FACTOR=1 # 2 ou 3 em prod
      - KAFKA_ACK_BATCH_SIZE=1000
      # =============================
      - KAFKA_WRITER_BATCH_SIZE=5000
      - KAFKA_WRITER_BATCH_BYTES=4194304 # 4MB
      - KAFKA_WRITER_BATCH_TIMEOUT_MS=25
      - KAFKA_WRITER_REQUIRED_ACKS=one
      - KAFKA_WRITER_MAX_ATTEMPTS=10
      # =============================
      - KAFKA_READER_MIN_BYTES=262144 # 256KB
      - KAFKA_READER_MAX_BYTES=5242880 # 5MB 
      - KAFKA_READER_MAX_WAIT_MS=150
      - KAFKA_READER_QUEUE_CAPACITY=20000
      - KAFKA_READER_READ_LAG_INTERVAL_MS=-1 # desabilitado
      - KAFKA_READER_SESSION_TIMEOUT_MS=30000
      - KAFKA_READER_HEARTBEAT_INTERVAL_MS=10000
      - KAFKA_READER_REBALANCE_TIMEOUT_MS=60000
      - KAFKA_READER_READ_BACKOFF_MIN_MS=100
      - KAFKA_READER_READ_BACKOFF_MAX_MS=1000
      # =============================
      - PROCESSING_WORKERS=8
      # =============================
      - REDIS_ADDR=redis:6379
      - REDIS_DB=0
      - REDIS_NAMESPACE=schema
      - REDIS_USE_PUBSUB=true
      - REDIS_INVALIDATE_CHANNEL=schemas:invalidate
    depends_on:
      collector:
        condition: service_started
      redis:
        condition: service_healthy
      schema-loader:
        condition: service_completed_successfully

  enricher:
    build: ./enricher
    container_name: enricher
    restart: unless-stopped
    environment:
      - KAFKA_BROKERS=kafka:9092
      - KAFKA_GROUP_ID=enricher
      - KAFKA_READER_TOPIC=validated-events-topic
      - KAFKA_WRITER_TOPIC=enriched-events-topic
      - KAFKA_DLQ_TOPIC=enriched-events-dlq
      - KAFKA_TOPIC_PARTITIONS=6
      - KAFKA_DLQ_PARTITIONS=1
      - KAFKA_RETENTION_MS=86400000
      - KAFKA_COMPRESSION=snappy
      - KAFKA_REPLICATION_FACTOR=1 # 2 ou 3 em prod
      - KAFKA_ACK_BATCH_SIZE=1000
      # =============================
      - KAFKA_WRITER_BATCH_SIZE=5000
      - KAFKA_WRITER_BATCH_BYTES=4194304 # 4MB
      - KAFKA_WRITER_BATCH_TIMEOUT_MS=25
      - KAFKA_WRITER_REQUIRED_ACKS=one
      - KAFKA_WRITER_MAX_ATTEMPTS=10
      # =============================
      - KAFKA_READER_MIN_BYTES=262144 # 256KB
      - KAFKA_READER_MAX_BYTES=5242880 # 5MB 
      - KAFKA_READER_MAX_WAIT_MS=150
      - KAFKA_READER_QUEUE_CAPACITY=20000
      - KAFKA_READER_READ_LAG_INTERVAL_MS=-1 # desabilitado
      - KAFKA_READER_SESSION_TIMEOUT_MS=30000
      - KAFKA_READER_HEARTBEAT_INTERVAL_MS=10000
      - KAFKA_READER_REBALANCE_TIMEOUT_MS=60000
      - KAFKA_READER_READ_BACKOFF_MIN_MS=100
      - KAFKA_READER_READ_BACKOFF_MAX_MS=1000
      # =============================
      - PROCESSING_WORKERS=8
      # =============================
      - CONTEXT_STORE_PATH=/app/device-context.json
    depends_on:
      validator:
        condition: service_started
      kafka:
        condition: service_healthy

  real-time-loader:
    build: ./real-time-loader
    container_name: real-time-loader
    restart: unless-stopped
    environment:
      - KAFKA_BROKERS=kafka:9092
      - KAFKA_GROUP_ID=real-time-loader
      - KAFKA_READER_TOPIC=enriched-events-topic
      - KAFKA_TOPIC_PARTITIONS=6
      - KAFKA_RETENTION_MS=86400000
      - KAFKA_COMPRESSION=snappy
      - KAFKA_REPLICATION_FACTOR=1 # 2 ou 3 em prod
      - KAFKA_ACK_BATCH_SIZE=1000
      # =============================
      - KAFKA_READER_MIN_BYTES=262144 # 256KB
      - KAFKA_READER_MAX_BYTES=5242880 # 5MB 
      - KAFKA_READER_MAX_WAIT_MS=150
      - KAFKA_READER_QUEUE_CAPACITY=20000
      - KAFKA_READER_READ_LAG_INTERVAL_MS=-1 # desabilitado
      - KAFKA_READER_SESSION_TIMEOUT_MS=30000
      - KAFKA_READER_HEARTBEAT_INTERVAL_MS=10000
      - KAFKA_READER_REBALANCE_TIMEOUT_MS=60000
      - KAFKA_READER_READ_BACKOFF_MIN_MS=100
      - KAFKA_READER_READ_BACKOFF_MAX_MS=1000
      # =============================
      - PROCESSING_WORKERS=8
      # =============================
      - INFLUX_URL=http://influxdb:8086
      - INFLUX_TOKEN=dev-token
      - INFLUX_ORG=hems
      - INFLUX_BUCKET=hems_timeseries
      - INFLUX_BATCH_SIZE=5000
      - INFLUX_FLUSH_INTERVAL_MS=100
    depends_on:
      kafka:
        condition: service_healthy
      influxdb:
        condition: service_healthy

  batch-loader:
    build: ./batch-loader
    container_name: batch-loader
    restart: unless-stopped
    environment:
      - KAFKA_BROKERS=kafka:9092
      - KAFKA_GROUP_ID=batch-loader
      - KAFKA_READER_TOPIC=enriched-events-topic
      - KAFKA_TOPIC_PARTITIONS=6
      - KAFKA_RETENTION_MS=86400000
      - KAFKA_COMPRESSION=snappy
      - KAFKA_REPLICATION_FACTOR=1 # 2 ou 3 em prod
      - KAFKA_ACK_BATCH_SIZE=1000
      # =============================
      - KAFKA_READER_MIN_BYTES=262144 # 256KB
      - KAFKA_READER_MAX_BYTES=5242880 # 5MB 
      - KAFKA_READER_MAX_WAIT_MS=150
      - KAFKA_READER_QUEUE_CAPACITY=20000
      - KAFKA_READER_READ_LAG_INTERVAL_MS=-1 # desabilitado
      - KAFKA_READER_SESSION_TIMEOUT_MS=30000
      - KAFKA_READER_HEARTBEAT_INTERVAL_MS=10000
      - KAFKA_READER_REBALANCE_TIMEOUT_MS=60000
      - KAFKA_READER_READ_BACKOFF_MIN_MS=100
      - KAFKA_READER_READ_BACKOFF_MAX_MS=1000
      # =============================
      - PROCESSING_WORKERS=8
      # =============================
      # ===== Batching (ajuste conforme o volume)
      - BATCH_MAX_RECORDS=100000
      - BATCH_MAX_BYTES=134217728 # 128MB
      - BATCH_MAX_INTERVAL_SEC=120
      # ===== MinIO / S3
      - S3_ENDPOINT=minio:9000
      - S3_ACCESS_KEY=minioadmin
      - S3_SECRET_KEY=minioadmin
      - S3_USE_TLS=false
      - S3_BUCKET=data-lakehouse
      - S3_BASE_PATH=bronze/hems
      # ===== Parquet
      - PARQUET_COMPRESSION=SNAPPY # ou ZSTD/GZIP
      # ===== Novas configs de tuning
      - MSG_CHANNEL_BUFFER=4096 # buffers de canais
      - RESULT_CHANNEL_BUFFER=4096
      - FLUSH_TICK_MS=1000
      - PARQUET_WRITER_PARALLEL=4 # paralelismo parquet (ajustar conforme CPU)
      - KAFKA_FETCH_RETRY_MS=50 # retry do fetch Kafka (ms)
      - KAFKA_COMMIT_TIMEOUT_MS=5000 # tempo limite para commit de mensagens (ms)
    depends_on:
      kafka:
        condition: service_healthy
      enricher:
        condition: service_started
      minio-init:
        condition: service_completed_successfully

volumes:
  kafka_data:
    driver: local
    driver_opts:
      type: none
      device: ./volumes/kafka_data
      o: bind
  influxdb_data:
    driver: local
    driver_opts:
      type: none
      device: ./volumes/influxdb_data
      o: bind
  minio_data:
    driver: local
    driver_opts:
      type: none
      device: ./volumes/minio_data
      o: bind