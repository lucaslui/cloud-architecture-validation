apiVersion: v1
kind: ConfigMap
metadata:
  name: batch-loader-config
  namespace: ingestion-environment-namespace
data:
  # ===== Kafka (consumer) =====
  KAFKA_BROKERS: "kafka:9092"
  KAFKA_GROUP_ID: "batch-loader"
  KAFKA_READER_TOPIC: "enriched-events-topic"
  KAFKA_TOPIC_PARTITIONS: "6"
  KAFKA_RETENTION_MS: "86400000"
  KAFKA_COMPRESSION: "snappy"
  KAFKA_REPLICATION_FACTOR: "1"
  KAFKA_ACK_BATCH_SIZE: "1000"

  # ===== Kafka Reader Tuning =====
  KAFKA_READER_MIN_BYTES: "262144"         # 256KB
  KAFKA_READER_MAX_BYTES: "5242880"        # 5MB
  KAFKA_READER_MAX_WAIT_MS: "150"
  KAFKA_READER_QUEUE_CAPACITY: "20000"
  KAFKA_READER_READ_LAG_INTERVAL_MS: "-1"  # desabilitado
  KAFKA_READER_SESSION_TIMEOUT_MS: "30000"
  KAFKA_READER_HEARTBEAT_INTERVAL_MS: "10000"
  KAFKA_READER_REBALANCE_TIMEOUT_MS: "60000"
  KAFKA_READER_READ_BACKOFF_MIN_MS: "100"
  KAFKA_READER_READ_BACKOFF_MAX_MS: "1000"
  KAFKA_FETCH_RETRY_MS: "50"
  KAFKA_COMMIT_TIMEOUT_MS: "5000"

  # ===== Processamento =====
  PROCESSING_WORKERS: "2"

  # ===== Batching =====
  BATCH_MAX_RECORDS: "100000"
  BATCH_MAX_BYTES: "134217728"   # 128MB
  BATCH_MAX_INTERVAL_SEC: "120"

  # ===== MinIO / S3 (somente não-sensíveis aqui) =====
  S3_ENDPOINT: "minio:9000"
  S3_USE_TLS: "false"
  S3_BUCKET: "data-lakehouse"
  S3_BASE_PATH: "bronze/hems"

  # ===== Parquet =====
  PARQUET_COMPRESSION: "SNAPPY"  # ou ZSTD/GZIP
  PARQUET_WRITER_PARALLEL: "4"

  # ===== Buffers e flush =====
  MSG_CHANNEL_BUFFER: "4096"
  RESULT_CHANNEL_BUFFER: "4096"
  FLUSH_TICK_MS: "1000"